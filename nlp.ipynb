{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1VciAlEqC-L"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "corpus = [\n",
        "    'do you like books?',\n",
        "    'this is a great book',\n",
        "    'the fit is great!',\n",
        "    'i love the shoes',\n",
        "]\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU8sQWfMwSzo"
      },
      "source": [
        "## Exploring Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHXRUI0WqGwD",
        "outputId": "3535372c-7541-499f-acee-380a3b42f8d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['do', 'you', 'like', 'books', '?']\n",
            "['?']\n",
            "['do', 'you']\n",
            "['like', 'book']\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(corpus[0])\n",
        "\n",
        "tokenization = []\n",
        "puct = []\n",
        "stop = []\n",
        "lemma = []\n",
        "\n",
        "for token in doc1:\n",
        "  tokenization.append(token.text)\n",
        "\n",
        "  if token.is_punct:\n",
        "    puct.append(token.text)\n",
        "\n",
        "  if token.is_stop:\n",
        "    stop.append(token.text)\n",
        "\n",
        "  if not token.is_punct and not token.is_stop:\n",
        "    lemma.append(token.lemma_)\n",
        "\n",
        "print(tokenization)\n",
        "print(puct)\n",
        "print(stop)\n",
        "print(lemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6-L1ExwhWQ"
      },
      "source": [
        "### Simple Text Preprocessing with Spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpaldnqqMed9",
        "outputId": "aad4f1cd-0a0e-41fd-cbbb-1cc267ab78a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "love book great book fit great love shoe\n"
          ]
        }
      ],
      "source": [
        "tokens_clean = []\n",
        "\n",
        "for sentence in corpus:\n",
        "  doc2 = nlp(sentence)\n",
        "\n",
        "  for token in doc2:\n",
        "\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "      tokens_clean.append(token.lemma_)\n",
        "\n",
        "text_clean = \" \".join(tokens_clean)\n",
        "\n",
        "print(text_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeHlE2D-RPSb"
      },
      "outputs": [],
      "source": [
        "Fiz o lammatization dos tokens, caso meu objetivo fosse jogar essas palavras em algum modelo de machine learning,\n",
        "eu ainda precisaria transformar o texto preprocessado em vetor para alimentar o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jm1McAOSfI0"
      },
      "source": [
        "### Other techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZkfzMiJptIvu",
        "outputId": "0d40880b-2622-4866-8e73-0a728ae2eb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting TextBlob\n",
            "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: nltk>=3.8 in /usr/local/lib/python3.10/dist-packages (from TextBlob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->TextBlob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->TextBlob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.8->TextBlob) (4.66.4)\n",
            "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: TextBlob\n",
            "  Attempting uninstall: TextBlob\n",
            "    Found existing installation: textblob 0.17.1\n",
            "    Uninstalling textblob-0.17.1:\n",
            "      Successfully uninstalled textblob-0.17.1\n",
            "Successfully installed TextBlob-0.18.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U TextBlob\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irxad3AFwm1J"
      },
      "source": [
        "## Exploring TextBlob Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6GIMi-cUZOY",
        "outputId": "5146f4ea-ebad-4a10-d6e4-14181da23b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TOKEN: ['my', 'hair', 'is', 'dri']\n",
            "TAGS: [('my', 'PRP$'), ('hair', 'NN'), ('is', 'VBZ'), ('dri', 'JJ')]\n",
            "CORRECTED SENTENCE: my hair is dry\n",
            "POLARITY:  0.5\n",
            "SUBJECTIVITY:  0.6\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "sentence = \"my hair is dri\"\n",
        "sentence2 = \"i love to study ai\"\n",
        "\n",
        "wiki = TextBlob(sentence)\n",
        "wiki2 = TextBlob(sentence2)\n",
        "\n",
        "#TextBlob Tokenization\n",
        "print(\"TOKEN:\", wiki.words)\n",
        "\n",
        "#Classify the words in part of speech\n",
        "print(\"TAGS:\", wiki.tags)\n",
        "\n",
        "#Correct the sentence\n",
        "print(\"CORRECTED SENTENCE:\", wiki.correct())\n",
        "\n",
        "#Sentiment Analysis\n",
        "##Measures if the sentiment is good or bad. -1 stands to bad feeling and 1 stands for a good feeling\n",
        "print(\"POLARITY: \", wiki2.sentiment.polarity)\n",
        "\n",
        "##How much of personal opnion the sentence contain\n",
        "print(\"SUBJECTIVITY: \", wiki2.sentiment.subjectivity)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
